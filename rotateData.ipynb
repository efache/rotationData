{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hKpNJcK5TXt7"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from google.colab import drive # Google Colab\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "import os\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "from math import ceil, floor\n",
        "from functools import reduce\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNCIcpKTUcOt"
      },
      "outputs": [],
      "source": [
        "# Add link to your drive repository\n",
        "linkDrive = \"To Modify1\" # Default link '/content/drive'\n",
        "if linkDrive == \"To Modify\":\n",
        "  raise ValueError(\"Modify link to your drive repository\")\n",
        "else:\n",
        "  drive.mount(linkDrive)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add link to your data repository in your drive\n",
        "folderPath = \"To Modify\" # Default link '/content/drive/your/path/to/your/data'\n",
        "if folderPath == \"To Modify\":\n",
        "  raise ValueError(\"Modify link to your data repository\")"
      ],
      "metadata": {
        "id": "keRqwhyEZ-fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnmp0054AKAI"
      },
      "source": [
        "## Data Rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yxvzDDoG-Mwu"
      },
      "outputs": [],
      "source": [
        "# Check Splitting \n",
        "# If gap between 2 split point is too small return false\n",
        "def checkDataSplitIsCorrect(splitPoints, lengthData, maxRotationDurationTs):\n",
        "  for i, point in enumerate(splitPoints[:-1]):\n",
        "    if splitPoints[i+1]-point <= maxRotationDurationTs + 1:\n",
        "      return False\n",
        "  if lengthData-1-splitPoints[-1] <= maxRotationDurationTs + 1:\n",
        "      return False\n",
        "  return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K6RCKIpMVJQf"
      },
      "outputs": [],
      "source": [
        "# Methode to create roation in data\n",
        "# data : raw data extract from txt file\n",
        "# minRot : minimum number of rotations done\n",
        "# maxRot : minimum number of rotations done\n",
        "# minRotationDurationTs : minimum duration of rotations (in Ts)\n",
        "# maxRotationDurationTs : maximum duration of rotations (in Ts)\n",
        "# Return :\n",
        "# dicRotation : all rotation information\n",
        "# dataRotate : data with rotation process\n",
        "# splitPoints : points where rotation done \n",
        "def createRotationInData (data, minRot = 10, maxRot = 11, minRotationDurationTs = 3, maxRotationDurationTs = 10) :\n",
        "  firstPass = True\n",
        "  dicRotation = {} # Stack rotation\n",
        "  dataRotate = [] # Stack data rotate\n",
        "  lengthData = data.shape[0] # Len data  \n",
        "  bufferRotation = R.from_rotvec(np.array([0, 0, 0]), degrees=True) # Rotation from the initial position \n",
        "  indexSplitSection = 0\n",
        "\n",
        "  # Split data \n",
        "  #Check if split data is correct\n",
        "  while firstPass or not checkDataSplitIsCorrect(splitPoints, lengthData, maxRotationDurationTs) :\n",
        "    firstPass = False\n",
        "    rotationCounter = random.randint(minRot, maxRot) # Number of rotation period n\n",
        "    splitPoints = np.random.choice(lengthData - 2, rotationCounter- 1, replace=False) + 1 # Random choice n-1 split point \n",
        "    splitPoints.sort() # Sort splitPoints\n",
        "    splitSections = np.split(data, splitPoints) # Split data in n sections\n",
        "    splitPoints = np.insert(splitPoints,0,0) # Add 0 in splitPoints \n",
        "\n",
        "  for splitSection, splitPoint in zip(splitSections,splitPoints):\n",
        "    indexSplitSection += 1 \n",
        "    cumulatingRotation = [] # Buffer to stock cumulative rotation \n",
        "    firstElements = [] # Buffer to stock first elements\n",
        "\n",
        "    # Compute rotation\n",
        "    rotationTimeStampDuration = random.randint(minRotationDurationTs,maxRotationDurationTs) # Random number nR of rotation duration in [|3,10|]\n",
        "    rots = R.random(rotationTimeStampDuration) # Calculate nR random rotations\n",
        "    rotSplitSection = bufferRotation.apply(splitSection[:,1:]) #Apply precedent rotation\n",
        "    for rot in rots: #Apply nR rotation in steps\n",
        "      #  Example Rotation apply for nR = 3 : [0, r1, r2*r1, r3*r2*r1, r3*r2,r1, ...]\n",
        "      firstElements.append(rotSplitSection[0]) # Save first element\n",
        "      cumulatingRotation.append(bufferRotation) # Save cumulative rotation for this Ts\n",
        "      rotSplitSection = rot.apply(rotSplitSection[1:,:]) #Apply rotation \n",
        "      bufferRotation = rot*bufferRotation # Save total rotation (the order is important)\n",
        "\n",
        "    cumulatingRotation.append(bufferRotation) # Add las rotate in buffer\n",
        "    rotSplitSection = np.concatenate((firstElements, rotSplitSection)) # Add first element remove during rotation\n",
        "    rotSplitSectionWithTs = [ np.concatenate((e1, e2))for e1,e2 in zip(np.reshape(splitSection[:,0],(-1,1)),rotSplitSection)] # Add Ts in rotate Coord\n",
        "    dicRotation[splitPoint] = cumulatingRotation \n",
        "    dataRotate.append(rotSplitSectionWithTs)\n",
        "  return dicRotation, np.array(dataRotate,dtype=object), splitPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "41rKCfArCQ7n"
      },
      "outputs": [],
      "source": [
        "# Methode to create dataframe from raw data and rotate data\n",
        "# data : raw data extract from txt file\n",
        "# finename : filename\n",
        "# dataRotate : data with rotation\n",
        "# dicRotation : all rotation information in data\n",
        "# Return :\n",
        "# pandas dataframe with all information necessary for preprocessing\n",
        "def createDataFrameFromFile(data, filename, dicRotation, dataRotate):\n",
        "  lengthData = data.shape[0] # Len data  \n",
        "  indexFileList = []\n",
        "  indexFile = int(filename.split(\"_\")[1].split(\".\")[0])\n",
        "  dataRotateTs =  []\n",
        "  dataRotateX = []\n",
        "  dataRotateY = []\n",
        "  dataRotateZ = []\n",
        "  dataX = []\n",
        "  dataY = []\n",
        "  dataZ = []\n",
        "  for section in dataRotate:\n",
        "    for track in section : \n",
        "      indexFileList.append(filename)\n",
        "      dataRotateTs.append(track[0])\n",
        "      dataRotateX.append(track[1])\n",
        "      dataRotateY.append(track[2])\n",
        "      dataRotateZ.append(track[3])\n",
        "  \n",
        "  indexRot = 0\n",
        "  keyDictRotation = list(dicRotation.keys())\n",
        "  keyDictRotation.append(lengthData)\n",
        "  rotationTab = []\n",
        "  for index, splitPoint in enumerate(keyDictRotation[:-1]):\n",
        "    for el in dicRotation[splitPoint]:\n",
        "      rotationTab.append(el)\n",
        "      indexRot+=1\n",
        "    while indexRot<keyDictRotation[index+1]:\n",
        "      rotationTab.append(rotationTab[-1])\n",
        "      indexRot+=1\n",
        "  for trackData in data : \n",
        "      dataX.append(trackData[1])\n",
        "      dataY.append(trackData[2])\n",
        "      dataZ.append(trackData[3])\n",
        "  d = {\"File\" : indexFile, \"Ts\":dataRotateTs, \"X\" : dataRotateX, \"Y\" : dataRotateY, \"Z\" : dataRotateZ, \"Rotation\" : rotationTab, \"XTarget\" : dataX, \"YTarget\" : dataY, \"ZTarget\" : dataZ}\n",
        "  return pd.DataFrame(data = d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0hUcp3FAXS7"
      },
      "outputs": [],
      "source": [
        "# Get all data from all file\n",
        "d = {\"File\" : [1], \"Ts\":[1], \"X\" : [1], \"Y\" : [1], \"Z\" : [1], \"Rotation\" : [1]}\n",
        "df = pd.DataFrame(data = d)\n",
        "for path, dirs, files in os.walk(folderPath):\n",
        "    for filename in files:\n",
        "      print(\"File in process :\",filename)\n",
        "      data = np.loadtxt(folderPath + filename) # Get data from file\n",
        "      dicRotation, dataRotate, splitPoints = createRotationInData(data) # Rotate the data\n",
        "      dfTemp = createDataFrameFromFile(data, filename, dicRotation, dataRotate) # Create dataframe from raw data and rotate data\n",
        "      df = df.append(dfTemp)\n",
        "\n",
        "df = df.reset_index()\n",
        "df = df.drop(0)\n",
        "df = df.reset_index()\n",
        "df = df.drop(columns=[\"index\",\"level_0\"])\n",
        "dfTemp = None # Free variable\n",
        "# df.to_csv(\"dataTracking.csv\")  # Uncomment to get data in CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MU486HycqRGw"
      },
      "outputs": [],
      "source": [
        "# Check if rotation script is ok\n",
        "# Useful only if you want to check that everything has been done correctly\n",
        "def checkRotation(dicRotation, dataRotate, data, indexSection = 0, limitPrint = 200, splitPoints = splitPoints) :\n",
        "  #Complete dic rotation\n",
        "  indexAVerif = splitPoints[indexSection]\n",
        "  print(\"Nombre rotation = \", len(dicRotation[indexAVerif]))\n",
        "  print(\"Nombre data a rotate = \", len(dataRotate[indexSection]))\n",
        "  rotationTab = deepcopy(dicRotation[indexAVerif])\n",
        "  while len(rotationTab) < len(dataRotate[indexSection]):\n",
        "    rotationTab.append(dicRotation[indexAVerif][-1])\n",
        "  index = 0 + indexAVerif\n",
        "  for dataRotateCoord, rotation in zip(dataRotate[indexSection][0:limitPrint], rotationTab[0:limitPrint]):\n",
        "    rot = rotation.inv()\n",
        "    print(data[index], \"------>\", [float(int(e)) for e in rot.apply(dataRotateCoord[1:])])\n",
        "    index +=1\n",
        "\n",
        "#checkRotation(dicRotation, dataRotate, data, indexSection = 2) Uncomment to check Rotation Script "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF9Klvii_YNo"
      },
      "source": [
        "## Analyse & preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3wNGFnd0_XxD"
      },
      "outputs": [],
      "source": [
        "# Preprocess Data\n",
        "# df : dataframe create before (column same as df create by createDataFrameFromFile function)\n",
        "# Return:\n",
        "# df : pandas dataframe with only necessary columns and data normalise\n",
        "# normalizeInfos : information to recover original data  \n",
        "def preprocessData(df):\n",
        "  X = []\n",
        "  Y = []\n",
        "  files = df[\"File\"].unique() # Get list Files\n",
        "\n",
        "  # Get min, max for each coord in data\n",
        "  minX, minY, minZ = min(floor(df[\"X\"].min()),floor(df[\"XTarget\"].min())), min(floor(df[\"Y\"].min()),floor(df[\"YTarget\"].min())), min(floor(df[\"Z\"].min()),floor(df[\"ZTarget\"].min()))\n",
        "  maxX, maxY, maxZ = max(ceil(df[\"X\"].max()), ceil(df[\"XTarget\"].max())), max(ceil(df[\"Y\"].max()),ceil(df[\"YTarget\"].max())), max(ceil(df[\"Z\"].max()),ceil(df[\"ZTarget\"].max()))\n",
        "  maxAbsX, maxAbsY, maxAbsz = max(abs(minX),maxX),  max(abs(minY),maxY),  max(abs(minZ),maxZ),\n",
        "\n",
        "  # Normalize \n",
        "  df[\"X\"], df[\"Y\"], df[\"Z\"] = df[\"X\"]/ maxAbsX, df[\"Y\"]/ maxAbsY, df[\"Z\"]/ maxAbsz\n",
        "  df[\"XTarget\"], df[\"YTarget\"], df[\"ZTarget\"] = df[\"XTarget\"]/ maxAbsX, df[\"YTarget\"]/ maxAbsY, df[\"ZTarget\"]/ maxAbsz\n",
        "\n",
        "  # Add a column with the original file of the data\n",
        "  for file in files:\n",
        "    dfFile = df[df[\"File\"] == file]\n",
        "  normalizeInfos = pd.DataFrame(data = {\"maxAbsX\" : [maxAbsX], \"maxAbsY\" : [maxAbsY], \"maxAbsz\" : [maxAbsz]})\n",
        "\n",
        "  # Group x,y,z coordinates in array\n",
        "  dataInput=[]\n",
        "  for x,y,z in zip(df[\"X\"], df[\"Y\"], df[\"Z\"]):\n",
        "    dataInput.append([x, y,z])\n",
        "  df[\"input\"] = dataInput\n",
        "\n",
        "  dataOutput=[]\n",
        "  for x,y,z in zip(df[\"XTarget\"], df[\"YTarget\"], df[\"ZTarget\"]):\n",
        "    dataOutput.append([x, y,z])\n",
        "  df[\"output\"] = dataOutput\n",
        "  df.drop(columns=[\"XTarget\", \"YTarget\", \"ZTarget\", \"X\", \"Y\", \"Z\", \"Rotation\"], inplace=True)\n",
        "  return df, normalizeInfos\n",
        "\n",
        "#Checkpoint copy, comment to release cache\n",
        "dfTest = deepcopy(df)\n",
        "dfNorm, normalizeInfos = preprocessData(dfTest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AJL3kubonCOs"
      },
      "outputs": [],
      "source": [
        "# Split data in test-train\n",
        "fileTrain = dfNorm[\"File\"].unique()[:-2]\n",
        "fileTest = dfNorm[\"File\"].unique()[-2:]\n",
        "dfTest = dfNorm[np.logical_or.reduce([dfNorm[\"File\"]==fileTest[0],dfNorm[\"File\"]==fileTest[1]])]  \n",
        "dfTrain = dfNorm[np.logical_not(np.logical_or.reduce([dfNorm[\"File\"]==fileTest[0],dfNorm[\"File\"]==fileTest[1]]))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZwNaU-oUOe1X"
      },
      "outputs": [],
      "source": [
        "# Sequence objet to avoid storing everything in cache\n",
        "class Sequence(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, df, sequenceLength):\n",
        "        # Initialization\n",
        "        self.sequenceLength = sequenceLength\n",
        "        self.x = np.array(df[\"input\"].tolist())\n",
        "        self.y = np.array(df[\"output\"].tolist())\n",
        "        self.datalen = len(self.y)\n",
        "        self.indexes = np.arange(self.datalen)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get batch indexes from shuffled indexes\n",
        "        batch_indexes = self.indexes[index*self.sequenceLength:(index+1)*self.sequenceLength]\n",
        "        while len(dfNorm.iloc[batch_indexes][\"File\"].unique())!=1: # Check all indexes are from the same file \n",
        "          index+=1\n",
        "          batch_indexes = self.indexes[index*self.sequenceLength:(index+1)*self.sequenceLength]\n",
        "        x_batch = np.reshape(self.x[batch_indexes], (1, sequenceLength, 3))\n",
        "        y_batch = np.reshape(self.y[batch_indexes], (1, sequenceLength, 3))\n",
        "        return x_batch, y_batch\n",
        "    \n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch\n",
        "        return self.datalen // self.sequenceLength\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updates indexes after each epoch\n",
        "        self.indexes = np.arange(self.datalen)\n",
        "\n",
        "# Create train and test sequence for training\n",
        "sequenceLength = 20\n",
        "train = Sequence(df = dfTrain, sequenceLength = sequenceLength)\n",
        "test = Sequence(df = dfTest, sequenceLength = sequenceLength)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Creation- Training and Prediction\n"
      ],
      "metadata": {
        "id": "JUBg-55McdXW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1XHsMZU2l3Ta"
      },
      "outputs": [],
      "source": [
        "class AutoencoderCNN1D(Model):\n",
        "\n",
        "  def __init__(self, latent_dim, n_filters_latent, filter_size, sequenceLength, stridesTab):\n",
        "    super(AutoencoderCNN1D, self).__init__()\n",
        "    self.n_filters_latent = n_filters_latent   \n",
        "    self.filter_size = filter_size  \n",
        "    maxFilter = n_filters_latent * 2 ** (depth-1) \n",
        "    lengthSeqBeforeLatent = int(sequenceLength / int(reduce(lambda x,y: x*y, stridesTab)))\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Input(shape=(sequenceLength, 3)),\n",
        "      layers.Conv1D(filters = n_filters_latent* 2 ** (depth-2) , kernel_size = filter_size, padding=\"same\", strides=stridesTab[0], activation=\"relu\"),\n",
        "      layers.Conv1D(filters = n_filters_latent * 2 ** (depth-1), kernel_size = filter_size , padding=\"same\", strides=stridesTab[1], activation=\"relu\"),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(latent_dim),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.InputLayer(input_shape=(latent_dim,)),\n",
        "      layers.Dense(units=lengthSeqBeforeLatent*maxFilter, activation=\"relu\"),\n",
        "      layers.Reshape(target_shape=(lengthSeqBeforeLatent, maxFilter)),\n",
        "      layers.Conv1DTranspose(filters = n_filters_latent * 2 ** (depth-1), kernel_size= filter_size, padding=\"same\", strides=2, activation=\"relu\" ),\n",
        "      layers.Conv1DTranspose(filters = n_filters_latent * 2 ** (depth-2), kernel_size = filter_size, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "      layers.Conv1DTranspose(filters= 3, kernel_size=1, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "latent_dim = 8 \n",
        "n_filters_latent = 16\n",
        "filter_size = 3\n",
        "stridesTab = [2,2]\n",
        "depth = 2\n",
        "# Model Creation\n",
        "autoencoder = AutoencoderCNN1D(latent_dim, n_filters_latent, filter_size, sequenceLength, stridesTab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecgsbT-KlOFz"
      },
      "outputs": [],
      "source": [
        "# Compilation\n",
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError(), metrics = [\"mean_squared_error\"])\n",
        "\n",
        "# Early Stopping to avoid overfitting\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=5,\n",
        "                                                    mode='min',\n",
        "                                                    min_delta=0.0001)\n",
        "# Training\n",
        "autoencoder.fit(train, validation_data = test, callbacks = early_stopping,  verbose=1, batch_size = 64, epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SYVOx6TfGKWd"
      },
      "outputs": [],
      "source": [
        "prediction = autoencoder.predict(test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}